{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file shows the complete process of developing the model from preprocessing to training and finding suitable parameters\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First, read the csv file and drop unused columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function drops unused columns, calculates the mean of collinear variables and adds a new column \"month\" describing in which month the observation was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def transform_data_median(data):\n",
    "    def mean_by_height(substring):\n",
    "        return data.filter(like = substring).filter(like = \".mean\").median(axis = 1)\n",
    "    \n",
    "    def std_by_height(substring):\n",
    "        return data.filter(like = substring).filter(like = \".std\").median(axis = 1)\n",
    "    \n",
    "    data = data.drop([\"id\", \"partlybad\"], axis = 1)\n",
    "    class2 = np.array([\"nonevent\", \"event\"])\n",
    "    data.insert(1, \"class2\", class2[(data[\"class4\"] != \"nonevent\").astype(int)])\n",
    "\n",
    "    new_data = pd.DataFrame()\n",
    "    new_data[['date', 'class2']] = data[['date', 'class2']]\n",
    "    new_data[\"CO.mean\"] = mean_by_height(\"CO\")\n",
    "    new_data[\"CO.std\"] = std_by_height(\"CO\")\n",
    "    new_data[\"H2O.mean\"] = mean_by_height(\"H2O\")\n",
    "    new_data[\"H2O.std\"] = std_by_height(\"H2O\")\n",
    "    new_data[\"RHIRGA.mean\"] = mean_by_height(\"RHIRGA\")\n",
    "    new_data[\"RHIRGA.std\"] = std_by_height(\"RHIRGA\")\n",
    "    new_data[\"NOx.mean\"] = mean_by_height(\"NOx\")\n",
    "    new_data[\"NOx.std\"] = std_by_height(\"NOx\")\n",
    "    new_data[\"NET.mean\"] = mean_by_height(\"NET\")\n",
    "    new_data[\"NET.std\"] = std_by_height(\"NET\")\n",
    "    new_data[\"NO.mean\"] = data.iloc[:,27:39].filter(like = \".mean\").median(axis = 1)\n",
    "    new_data[\"NO.std\"] = data.iloc[:,27:39].filter(like = \".std\").median(axis = 1)\n",
    "    new_data[\"O3.mean\"] = data.iloc[:,51:61].filter(like = \".mean\").median(axis = 1)\n",
    "    new_data[\"O3.std\"] = data.iloc[:, 51:61].filter(like = \".std\").median(axis = 1)\n",
    "    new_data[['Pamb0.mean', 'Pamb0.std', 'PAR.mean', 'PAR.std', 'PTG.mean', 'PTG.std', 'RGlob.mean', 'RGlob.std']] = data[['Pamb0.mean', 'Pamb0.std', 'PAR.mean', 'PAR.std', 'PTG.mean', 'PTG.std', 'RGlob.mean', 'RGlob.std']]\n",
    "    new_data[['RPAR.mean', 'RPAR.std', 'SO2168.mean', 'SO2168.std', 'SWS.mean', 'SWS.std']] = data[['RPAR.mean', 'RPAR.std', 'SO2168.mean', 'SO2168.std', 'SWS.mean', 'SWS.std']]\n",
    "    new_data[\"T.mean\"] = data.filter(like = \"T\").iloc[:,4:].filter(like = \".mean\").median(axis = 1)\n",
    "    new_data[\"T.std\"] = data.filter(like = \"T\").iloc[:,4:].filter(like = \".std\").median(axis = 1)\n",
    "    new_data[['UV_A.mean', 'UV_A.std', 'UV_B.mean', 'UV_B.std', 'CS.mean', 'CS.std']] = data[['UV_A.mean', 'UV_A.std', 'UV_B.mean', 'UV_B.std', 'CS.mean', 'CS.std']]\n",
    "    new_data.set_index('date')\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, transform the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>class2</th>\n",
       "      <th>CO.mean</th>\n",
       "      <th>CO.std</th>\n",
       "      <th>H2O.mean</th>\n",
       "      <th>H2O.std</th>\n",
       "      <th>RHIRGA.mean</th>\n",
       "      <th>RHIRGA.std</th>\n",
       "      <th>NOx.mean</th>\n",
       "      <th>NOx.std</th>\n",
       "      <th>...</th>\n",
       "      <th>SWS.mean</th>\n",
       "      <th>SWS.std</th>\n",
       "      <th>T.mean</th>\n",
       "      <th>T.std</th>\n",
       "      <th>UV_A.mean</th>\n",
       "      <th>UV_A.std</th>\n",
       "      <th>UV_B.mean</th>\n",
       "      <th>UV_B.std</th>\n",
       "      <th>CS.mean</th>\n",
       "      <th>CS.std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-17</td>\n",
       "      <td>event</td>\n",
       "      <td>368.718684</td>\n",
       "      <td>0.307718</td>\n",
       "      <td>4.370921</td>\n",
       "      <td>0.145338</td>\n",
       "      <td>74.526311</td>\n",
       "      <td>3.099995</td>\n",
       "      <td>0.630081</td>\n",
       "      <td>0.084554</td>\n",
       "      <td>...</td>\n",
       "      <td>937.880000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.996623</td>\n",
       "      <td>0.260461</td>\n",
       "      <td>2.492491</td>\n",
       "      <td>1.310880</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>nonevent</td>\n",
       "      <td>378.140192</td>\n",
       "      <td>1.019733</td>\n",
       "      <td>7.195852</td>\n",
       "      <td>0.196708</td>\n",
       "      <td>99.990803</td>\n",
       "      <td>0.930678</td>\n",
       "      <td>3.833197</td>\n",
       "      <td>0.833653</td>\n",
       "      <td>...</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.911915</td>\n",
       "      <td>0.279675</td>\n",
       "      <td>0.295937</td>\n",
       "      <td>0.177836</td>\n",
       "      <td>0.005140</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>event</td>\n",
       "      <td>372.986612</td>\n",
       "      <td>0.739138</td>\n",
       "      <td>3.564013</td>\n",
       "      <td>0.254880</td>\n",
       "      <td>57.290658</td>\n",
       "      <td>13.988569</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.196336</td>\n",
       "      <td>...</td>\n",
       "      <td>923.745098</td>\n",
       "      <td>2.161880</td>\n",
       "      <td>0.541530</td>\n",
       "      <td>2.226999</td>\n",
       "      <td>14.434789</td>\n",
       "      <td>8.627312</td>\n",
       "      <td>0.353743</td>\n",
       "      <td>0.272472</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>event</td>\n",
       "      <td>375.596225</td>\n",
       "      <td>0.549797</td>\n",
       "      <td>6.512460</td>\n",
       "      <td>0.511737</td>\n",
       "      <td>68.153041</td>\n",
       "      <td>8.804628</td>\n",
       "      <td>2.297301</td>\n",
       "      <td>0.485434</td>\n",
       "      <td>...</td>\n",
       "      <td>925.622642</td>\n",
       "      <td>1.389887</td>\n",
       "      <td>6.339887</td>\n",
       "      <td>2.650631</td>\n",
       "      <td>16.077513</td>\n",
       "      <td>9.984686</td>\n",
       "      <td>0.568242</td>\n",
       "      <td>0.451830</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>nonevent</td>\n",
       "      <td>377.635303</td>\n",
       "      <td>0.420603</td>\n",
       "      <td>6.223485</td>\n",
       "      <td>0.202019</td>\n",
       "      <td>88.604606</td>\n",
       "      <td>8.204318</td>\n",
       "      <td>3.069394</td>\n",
       "      <td>0.878080</td>\n",
       "      <td>...</td>\n",
       "      <td>921.727273</td>\n",
       "      <td>2.578074</td>\n",
       "      <td>1.885328</td>\n",
       "      <td>1.551858</td>\n",
       "      <td>9.710422</td>\n",
       "      <td>7.054069</td>\n",
       "      <td>0.339135</td>\n",
       "      <td>0.291457</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    class2     CO.mean    CO.std  H2O.mean   H2O.std  \\\n",
       "0  2000-01-17     event  368.718684  0.307718  4.370921  0.145338   \n",
       "1  2000-02-28  nonevent  378.140192  1.019733  7.195852  0.196708   \n",
       "2  2000-03-24     event  372.986612  0.739138  3.564013  0.254880   \n",
       "3  2000-03-30     event  375.596225  0.549797  6.512460  0.511737   \n",
       "4  2000-04-04  nonevent  377.635303  0.420603  6.223485  0.202019   \n",
       "\n",
       "   RHIRGA.mean  RHIRGA.std  NOx.mean   NOx.std  ...    SWS.mean   SWS.std  \\\n",
       "0    74.526311    3.099995  0.630081  0.084554  ...  937.880000  0.600000   \n",
       "1    99.990803    0.930678  3.833197  0.833653  ...  936.000000  0.707107   \n",
       "2    57.290658   13.988569  0.909836  0.196336  ...  923.745098  2.161880   \n",
       "3    68.153041    8.804628  2.297301  0.485434  ...  925.622642  1.389887   \n",
       "4    88.604606    8.204318  3.069394  0.878080  ...  921.727273  2.578074   \n",
       "\n",
       "     T.mean     T.std  UV_A.mean  UV_A.std  UV_B.mean  UV_B.std   CS.mean  \\\n",
       "0 -0.996623  0.260461   2.492491  1.310880   0.031587  0.018122  0.000243   \n",
       "1  1.911915  0.279675   0.295937  0.177836   0.005140  0.003552  0.003658   \n",
       "2  0.541530  2.226999  14.434789  8.627312   0.353743  0.272472  0.000591   \n",
       "3  6.339887  2.650631  16.077513  9.984686   0.568242  0.451830  0.002493   \n",
       "4  1.885328  1.551858   9.710422  7.054069   0.339135  0.291457  0.004715   \n",
       "\n",
       "     CS.std  \n",
       "0  0.000035  \n",
       "1  0.000940  \n",
       "2  0.000191  \n",
       "3  0.000466  \n",
       "4  0.000679  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = transform_data_median(pd.read_csv(\"npf_train.csv\"))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def transform_X(data):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(data = scaler.fit_transform(data), columns = data.columns)\n",
    "\n",
    "    pca = PCA(n_components = 12)\n",
    "    X_scaled = pd.DataFrame(data = pca.fit_transform(X_scaled))\n",
    "\n",
    "    return X_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "After preprocessing, we can start to develop the model. From previous testing, it was found that logistic regression is the best performer. But before that, we need to scale our data with standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train = train_data.iloc[:,2:]\n",
    "y_train = (train_data.iloc[:,1] == 'event').astype(float)\n",
    "\n",
    "X_train_scaled = transform_X(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating accuracy for binary classification\n",
    "\n",
    "To avoid overfitting, we reduce the dimensions with PCA. We will choose number of components with k-fold cross-validation on logistic regression. L2 regularization is used to lower the variance to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated accuracy of the model is: 0.8685133239831696\n",
      "The estimated MSE is 0.13148667601683028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "clf = LogisticRegression(penalty = 'l2', C = 1/4, solver = \"saga\", max_iter = 5000)\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf, X_train_scaled, y_train, cv = 5, scoring = ('accuracy', 'neg_mean_squared_error')\n",
    ")\n",
    "\n",
    "est_acc = scores['test_accuracy'].mean()\n",
    "mean_squared = -scores['test_neg_mean_squared_error'].mean()\n",
    "print(f'The estimated accuracy of the model is: {est_acc}')\n",
    "print(f'The estimated MSE is {mean_squared}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nonevent\n",
       "1       event\n",
       "2    nonevent\n",
       "3    nonevent\n",
       "4    nonevent\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = transform_data_median(pd.read_csv(\"npf_test_hidden.csv\"))\n",
    "\n",
    "X_test = test_data.iloc[:,2:]\n",
    "\n",
    "X_test_scaled = transform_X(X_test)\n",
    "\n",
    "predictions = pd.DataFrame(clf.fit(X_train_scaled, y_train).predict(X_test_scaled), columns = [\"class\"])\n",
    "predictions = predictions[\"class\"].map({1.0: 'event', 0.0:'nonevent'})\n",
    "predictions.to_csv(\"answers.csv\")\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class\n",
    "\n",
    "Transform the data in the same way, but keep the original classes. User random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651017576318224"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_randomForest = RandomForestClassifier()\n",
    "y_multiclass, uniques = pd.factorize(pd.read_csv(\"npf_train.csv\")[\"class4\"])\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf_randomForest, X_train, y_multiclass, cv = 10, scoring = ('accuracy', 'neg_mean_squared_error')\n",
    ")\n",
    "\n",
    "random_forest_predictions = pd.DataFrame(clf_randomForest.fit(X_train, y_multiclass).predict(X_test), columns = [\"class\"])\n",
    "random_forest_predictions = random_forest_predictions[\"class\"].map({0: 'Ib', 1:'nonevent', 2:'II', 3:'Ia'})\n",
    "random_forest_predictions.to_csv(\"multiclass_answers.csv\")\n",
    "\n",
    "scores[\"test_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ib', 'nonevent', 'II', 'Ia'], dtype='object')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
